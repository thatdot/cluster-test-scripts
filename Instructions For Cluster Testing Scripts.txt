Instructions for cluster testing scripts B*
--

These scripts exercise the following test cases: B1, B2, B3, B4, B5, and B6

These scripts are designed to be runnable on Quine Enterprise, regardless of cluster size. Each script includes a header describing its purpose and how to run it.
At the beginning of each file, most importantly, utils/prodenv.py, many variables are specified describing the environment against which the script should run. For example, the IPs of Quine cluster members and names of Kafka topics are specified via these variables. These variables should be updated to match the environment before running a script.

Files included:
b1.py
b3.py
b5.py
b6.py
data_checker.py
utils/prodenv.py (modified from original)
model/host_pb2.py (generated by protobuf)
host.proto (The descriptor for this schema is available at https://thatdot-public.s3.us-west-2.amazonaws.com/host.desc)
datagen.py
produce_messages.py

Summary of Scenarios:
- FB1: Ingest a large number of JSON files
- FB2: Ingest a large number of JSON files with Standing Queries
- B1-1 (b1.py): kafka/protobuf ingest with Standing Queries
- B1-2 (b1.py): kafka/protobuf ingest with late-registered Standing Queries
- B2-1 (b1.py + manual process killing): Kafka/protobuf ingest with cluster members being killed
- B2-2 (b1.py + manual process killing): Kafka/protobuf ingest with Standing Queries and cluster members being killed
- B3 (b3.py): kafka/protobuf ingest with unbounded Standing Queries
- B4 (b1.py + manual process killing): kafka/protobuf ingest with Standing Queries and cluster members being killed after a long period of stability
- B5 (b5.py): ad-hoc single queries against a supernode
- B6 (b6.py): ad-hoc batches of paginated queries against a supernode

Note on Script <-> Scenario mapping:
Not all scenarios have a script associated with them. In particular, FB1 and FB2 do not yet have scripts.
B2-1, B2-2, and B4 all use b1.py, as they use the same data pipeline from B1-1, and only differ from B1-1 in which processes are killed when.

Environment assumptions:
The host running the test script must be able to consume from Kafka, and have the following Python packages installed:
- requests
- kafka-python
- protobuf

Dataset assumptions:
The dataset used by these scenarios in various forms (JSON, protobuf, kafka, files) assumes the following:
- All records are protobuf Hosts (or the direct JSON analogue) as described by host.proto
- IPs are rarely or never reused for different hosts
- A Site is reused for several hosts, on the order of 1000 hosts per Site
- An OU is reused for many hosts, on the order of 100,000 hosts per OU
- A Customer is reused for a great many hosts, on the order of 1 million hosts per customer (this means that when they are defined, customer nodes are expected to be supernodes)
- For better throughput, the data should (but is not required to) be partitioned in Kafka by customer_id